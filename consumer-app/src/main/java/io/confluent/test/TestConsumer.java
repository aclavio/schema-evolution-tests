/*
 * This source file was generated by the Gradle 'init' task
 */
package io.confluent.test;

import io.confluent.kafka.schemaregistry.avro.AvroSchemaUtils;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.specific.SpecificData;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.errors.WakeupException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import stream.processing.demo.AccountCreated;
import stream.processing.demo.AccountDeleted;
import stream.processing.demo.AccountUpdated;

import java.io.FileInputStream;
import java.util.Arrays;
import java.util.List;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class TestConsumer implements Runnable {
    private static final Logger logger = LoggerFactory.getLogger(TestConsumer.class);

    private static final String DEFAULT_CONFIG_FILE = "consumer.properties";
    private static final String DEFAULT_TOPIC = "union.test.avro";

    private final KafkaConsumer<String, GenericRecord> consumer;
    //private final KafkaConsumer<String, AccountCreated> consumer;

    private final List<String> topics;
    private final CountDownLatch shutdownLatch;

    public TestConsumer(Properties config, List<String> topics) {
        // initialize the Kafka Consumer using the properties file
        this.consumer = new KafkaConsumer<>(config);
        this.topics = topics;
        this.shutdownLatch = new CountDownLatch(1);
    }

    @Override
    public void run() {
        try {
            // subscribe to the Kafka topics
            consumer.subscribe(topics);

            logger.info("Waiting for events...");

            // basic Kafka consumer "poll loop"
            while (true) {
                // poll for new kafka events
                //ConsumerRecords<String, AccountCreated> records = consumer.poll(Long.MAX_VALUE);
                ConsumerRecords<String, GenericRecord> records = consumer.poll(Long.MAX_VALUE);

                // application specific processing...
                records.forEach(record -> {
                    // for demo purposes, just emit a log statement
                    logger.info("[{} @{}] got record: [{}] {}",
                            record.topic(),
                            record.offset(),
                            record.key(),
                            record.value().toString());

                    // Convert the GenericRecord type from the union to the SpecificRecord type
                    GenericRecord genericRecord = record.value();
                    logger.info("record uses schema: {}", genericRecord.getSchema().getFullName());
                    if ("stream.processing.demo.AccountCreated".equals(genericRecord.getSchema().getFullName())) {
                        AccountCreated ac = (AccountCreated) SpecificData.get().deepCopy(AccountCreated.getClassSchema(), record.value());
                        logger.info("converted to specific record AccountCreated: {}", ac);
                    } else if ("stream.processing.demo.AccountDeleted".equals(genericRecord.getSchema().getFullName())) {
                        AccountDeleted ad = (AccountDeleted) SpecificData.get().deepCopy(AccountDeleted.getClassSchema(), record.value());
                        logger.info("converted to specific record AccountDeleted: {}", ad);
                    } else if ("stream.processing.demo.AccountUpdated".equals(genericRecord.getSchema().getFullName())) {
                        AccountUpdated au = (AccountUpdated) SpecificData.get().deepCopy(AccountUpdated.getClassSchema(), record.value());
                        logger.info("converted to specific record AccountUpdated: {}", au);
                    } else {
                        logger.warn("unexpected schema!");
                    }
                });

                // commit the offsets back to kafka
                consumer.commitSync();
            }

        } catch (WakeupException ex) {
            // awake from poll
        } catch (Exception ex) {
            logger.error("An unexpected error occurred!", ex);
        } finally {
            // gracefully shutdown the consumer!
            consumer.close();
            shutdownLatch.countDown();
        }
    }

    public void shutdown() throws InterruptedException {
        consumer.wakeup();
        shutdownLatch.await();
    }

    public static void main(String[] args) throws Exception {
        // load passed in properties
        String configPath = args.length > 0 ? args[0] : DEFAULT_CONFIG_FILE;
        final Properties cfg = new Properties();
        cfg.load(new FileInputStream(configPath));
        // get the topic to consumer from
        String kafkaTopic = args.length > 1 ? args[1] : DEFAULT_TOPIC;
        List<String> topics = Arrays.asList(kafkaTopic.split(","));

        // Start up our consumer thread
        TestConsumer bc = new TestConsumer(cfg, topics);
        Thread thread = new Thread(bc);
        thread.start();

        try {
            thread.join();
        } catch (InterruptedException e) {
            bc.shutdown();
        }
    }
}
